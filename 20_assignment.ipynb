{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5995286c-40a0-4245-a209-35e72126d99b",
   "metadata": {},
   "source": [
    "1) What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cbd412-1538-439a-98fb-b6d2098e0d1c",
   "metadata": {},
   "source": [
    "Web scraping refers to the process of extracting data from websites using automated software tools or scripts. It involves using programs to extract information from web pages, such as text, images, and links, and then transforming that information into a structured format that can be analyzed or used in other applications\n",
    "USES:\n",
    "1) Data Collection : Web scraping is often used to gather large amounts of data from websites that would otherwise be difficult or time-consuming to collect manually. This data can be used for market research, competitive analysis, or other business intelligence purposes\n",
    "2) Content Aggregation : Web scraping can be used to collect content from multiple websites and combine it into a single source, such as a news aggregator or price comparison site\n",
    "3) Resarch : Web scraping is commonly used in academic research to collect data for studies or to track changes in web content over time\n",
    "THREE AREAS : \n",
    "1) E-commerce : Retailers can use web scraping to collect data on competitors' prices, product listings, and customer reviews to inform their own pricing and marketing strategies\n",
    "2) Social-Media : Researchers can use web scraping to collect data from social media platforms, such as Twitter or Instagram, to analyze trends, track sentiment, and identify influencers\n",
    "3) Finance : Financial analysts can use web scraping to collect data on stock prices, market trends, and other financial metrics to inform their investment decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49316e07-59ae-4d97-8d36-9fbb2e49dc7d",
   "metadata": {},
   "source": [
    "2) What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803c4c29-1684-4512-996e-ac7f1a799022",
   "metadata": {},
   "source": [
    "1) Parsing HTML : This is the most basic method of web scraping, where the HTML source code of a web page \n",
    "is parsed using a programming language such as Python, and specific pieces of data are extracted based on \n",
    "their HTML tags, attributes, or CSS selectors\n",
    "2) DOM Parsing : The Document Object Model (DOM) is a representation of a web page as a hierarchical tree \n",
    "structure. This method involves parsing the DOM tree to extract data, using programming languages such as \n",
    "JavaScript or Python\n",
    "3) Web Scraping Libraries : There are several web scraping libraries available for various programming languages, \n",
    "such as Beautiful Soup and Scrapy for Python, or Cheerio for JavaScript. These libraries provide a range of \n",
    "functions and tools to extract data from websites "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d6b0d-d077-4672-b056-f4f989889b57",
   "metadata": {},
   "source": [
    "3) What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27865ab-db85-4bc5-a60f-53782fc85303",
   "metadata": {},
   "source": [
    "Beautiful Soup is a popular Python library used for web scraping purposes. It allows you to parse HTML and XML \n",
    "documents, navigate through the document tree, and extract the data you need. The library provides a simple \n",
    "and intuitive interface for working with HTML and XML data, making it a popular choice for developers and \n",
    "data scientists who need to extract data from websites\n",
    "1) Parsing HTML and XML : Beautiful Soup can be used to parse HTML and XML documents, making it easy to extract \n",
    "data from web pages\n",
    "2) Navigating the Document Tree : The library provides a range of functions for navigating the hierarchical \n",
    "structure of an HTML or XML document. You can use these functions to find specific elements, such as divs or \n",
    "spans, and extract their contents\n",
    "3) Searching and Filtering : Beautiful Soup allows you to search for specific elements based on their \n",
    "attributes or text content. You can also filter elements based on their position in the document tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a4fcd9-4a58-4b73-8c53-b6fd7a7cd7e6",
   "metadata": {},
   "source": [
    "4) Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a4a92-f78b-402d-83ff-035364951879",
   "metadata": {},
   "source": [
    "1) Easy To Use: Flask is a lightweight web framework that is easy to set up and use, making it a great choice \n",
    "for small to medium-sized web scraping projects\n",
    "2) Flexible : Flask is highly customizable, allowing developers to build web scraping applications that \n",
    "meet their specific needs\n",
    "3) Routing : Flask provides a routing system that allows developers to map URLs to specific functions, \n",
    "making it easy to build web scraping applications that can navigate to different web pages and extract data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc4fc2-10ce-42a1-8c3c-c207dcd8613c",
   "metadata": {},
   "source": [
    "5) Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229fc548-feb9-4c66-953a-b969f843af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1) pipeline : AWS Pipeline, also known as AWS CodePipeline, is a fully managed continuous delivery service \n",
    "that automates the release process for software applications. It allows developers to create, test, and deploy \n",
    "code changes continuously and automatically across multiple environments\n",
    "AWS Pipeline uses a series of stages, called actions, that are performed sequentially to build, test, \n",
    "and deploy code changes. These stages can be customized to meet the specific needs of the application and \n",
    "can include tasks such as building code, running unit tests, deploying to a staging environment, and promoting \n",
    "code to production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
