{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4054234a-f47f-4d1e-8638-6ef7304f020c",
   "metadata": {},
   "source": [
    "1) What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a77bd3-4322-46d1-99d1-b868901cb4f1",
   "metadata": {},
   "source": [
    "Anomaly detection is a process of identifying data points or events that deviate significantly from the expected or normal behavior within a given dataset. Anomalies can be caused by various factors, such as errors, fraud, cybersecurity attacks, equipment malfunctions, or changes in system behavior.\n",
    "\n",
    "The purpose of anomaly detection is to detect such anomalous events and outliers, which may otherwise go unnoticed in the vast amount of data generated by modern systems. By detecting anomalies, organizations can take appropriate action to prevent or mitigate potential issues, such as fraud, security breaches, or system failures, and improve overall system performance and reliability.\n",
    "\n",
    "Anomaly detection can be applied in various domains, including finance, healthcare, manufacturing, cybersecurity, and IoT, among others. The techniques used for anomaly detection can range from simple threshold-based methods to more advanced machine learning algorithms, such as clustering, classification, and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0539c17-8570-4249-8326-a90dac69b309",
   "metadata": {},
   "source": [
    "2) What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7f5387-b05f-406f-9f45-0fb05235e595",
   "metadata": {},
   "source": [
    "Anomaly detection poses several challenges, some of which include:\n",
    "\n",
    "1) Lack of labeled data: In many cases, anomalies are rare and occur infrequently, making it challenging to obtain labeled data to train anomaly detection algorithms.\n",
    "\n",
    "2) High dimensionality: The data generated by modern systems can be high-dimensional, making it difficult to identify anomalous patterns within the data.\n",
    "\n",
    "3) Unbalanced data: Anomalies are often a small fraction of the overall dataset, leading to class imbalance, which can make it challenging to train effective anomaly detection models.\n",
    "\n",
    "4) Concept drift: As the underlying system changes, the distribution of the data can also change, leading to concept drift, which can cause the model to lose its effectiveness over time.\n",
    "\n",
    "5) Noise: The presence of noise or outliers within the dataset can make it difficult to distinguish between anomalous and normal behavior.\n",
    "\n",
    "6) Computational complexity: The volume of data generated by modern systems can be massive, making it challenging to process and analyze the data in real-time.\n",
    "\n",
    "7) Interpretability: The output of many anomaly detection algorithms is often difficult to interpret, making it challenging to identify the root cause of the anomaly.\n",
    "\n",
    "Addressing these challenges requires a combination of domain expertise, statistical and machine learning techniques, and careful data preprocessing and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b4ac8-e774-4721-810e-5697cc1b72e7",
   "metadata": {},
   "source": [
    "3) How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb97e1-dcb6-454c-8ec7-9fd16ba2790a",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection are two broad approaches to detecting anomalies in data, which differ primarily in their use of labeled data during the detection process.\n",
    "\n",
    "Supervised anomaly detection relies on a dataset that is labeled as either normal or anomalous. The algorithm is trained on this labeled data to learn patterns that distinguish normal from anomalous behavior. Once the model is trained, it can be used to classify new data as normal or anomalous based on its similarity to the learned patterns.\n",
    "\n",
    "In contrast, unsupervised anomaly detection does not require labeled data and is used to identify anomalies based solely on the structure of the data itself. This approach assumes that the majority of the data is normal and seeks to identify outliers or anomalies that deviate significantly from this norm. Unsupervised methods can be useful in situations where it is difficult or impossible to obtain labeled data, or where anomalies may take on different forms over time.\n",
    "\n",
    "Some of the most common unsupervised anomaly detection techniques include statistical methods, such as clustering, density estimation, and dimensionality reduction, as well as machine learning methods, such as autoencoders and one-class SVMs.\n",
    "\n",
    "Overall, the choice of the anomaly detection approach depends on the availability and quality of labeled data, as well as the nature and complexity of the data being analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec9f1a1-a8a8-4f53-acd7-9d748713e3d3",
   "metadata": {},
   "source": [
    "4) What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076dd20d-2eae-44f5-bdda-9d91d4724fee",
   "metadata": {},
   "source": [
    "There are several categories of anomaly detection algorithms, each of which relies on different statistical and machine learning techniques to identify anomalies. Some of the most common categories include:\n",
    "\n",
    "1) Statistical methods: Statistical methods are based on the assumption that normal data points follow a specific statistical distribution. Anomalies are identified as data points that deviate significantly from this distribution. Examples of statistical methods include Z-score, Grubbs' test, and Dixon's Q-test.\n",
    "\n",
    "2) Machine learning-based methods: Machine learning algorithms can be used to identify anomalies by learning the underlying patterns in the data. These algorithms can be either supervised or unsupervised. Examples of machine learning-based methods include clustering algorithms, support vector machines, and neural networks.\n",
    "\n",
    "3) Density-based methods: Density-based methods identify anomalies by finding regions of low density in the data. Anomalies are points that lie in these regions of low density. Examples of density-based methods include Local Outlier Factor (LOF) and Isolation Forest.\n",
    "\n",
    "4) Proximity-based methods: Proximity-based methods identify anomalies by measuring the distance or similarity between data points. Anomalies are identified as data points that are significantly different from their neighbors. Examples of proximity-based methods include k-nearest neighbors and distance-based outliers.\n",
    "\n",
    "5) Spectral analysis: Spectral analysis is a technique that identifies anomalies by analyzing the spectral properties of the data. Anomalies are identified as data points that do not fit the spectral properties of the normal data. Spectral analysis is commonly used in signal processing and image analysis.\n",
    "\n",
    "6) Rule-based methods: Rule-based methods identify anomalies by using a set of pre-defined rules. These rules are often based on domain-specific knowledge and experience. Examples of rule-based methods include expert systems and decision trees.\n",
    "\n",
    "The choice of the anomaly detection algorithm depends on the nature and complexity of the data being analyzed, as well as the availability and quality of labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a6c704-fc83-461f-be32-393a19d00b4b",
   "metadata": {},
   "source": [
    "5) What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a179f5c1-3488-4aa3-832b-3e8c043911b7",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that normal data points are located close to each other, while anomalous data points are located far away from normal points. These methods typically use a distance metric, such as Euclidean distance or Mahalanobis distance, to measure the similarity between data points.\n",
    "\n",
    "The main assumptions made by distance-based anomaly detection methods are:\n",
    "\n",
    "1) Distance metric: These methods assume that a valid distance metric can be defined to measure the similarity between data points. The choice of distance metric depends on the nature and complexity of the data being analyzed.\n",
    "\n",
    "2) Normal data distribution: Distance-based methods assume that normal data points are distributed around a central cluster or manifold. Anomalies are identified as data points that lie outside this cluster or manifold.\n",
    "\n",
    "3) Single cluster: Distance-based methods assume that all normal data points belong to a single cluster. If the data contains multiple clusters, the algorithm may incorrectly identify some data points within a cluster as anomalies.\n",
    "\n",
    "4) Independence of features: Distance-based methods assume that the features used to measure the distance between data points are independent of each other. If the features are correlated, the algorithm may overweight some features and underweight others, leading to incorrect anomaly detection.\n",
    "\n",
    "5) Stationarity: Distance-based methods assume that the distribution of normal data remains stationary over time. If the distribution changes over time, the algorithm may fail to detect anomalies.\n",
    "\n",
    "6) Gaussian distribution: Some distance-based methods, such as the Mahalanobis distance, assume that the data follows a Gaussian distribution. If the data does not follow a Gaussian distribution, the algorithm may produce incorrect results.\n",
    "\n",
    "It is important to carefully consider these assumptions when choosing a distance-based anomaly detection method and evaluating its performance on the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7cafe1-11db-4b14-9cac-fc86d26df9c1",
   "metadata": {},
   "source": [
    "6) How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551af27-c594-4f95-ad5e-d80b2b2fe622",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm is a density-based anomaly detection method that computes an anomaly score for each data point based on its local density. The LOF algorithm uses the concept of k-nearest neighbors to measure the local density of each data point.\n",
    "\n",
    "To compute the anomaly score for a data point, the LOF algorithm performs the following steps:\n",
    "\n",
    "1) Compute the k-distance: The k-distance of a data point is defined as the distance to its k-th nearest neighbor.\n",
    "\n",
    "2) Compute the reachability distance: The reachability distance between two data points is defined as the maximum of the k-distance of the second data point and the distance between the two points.\n",
    "\n",
    "3) Compute the local reachability density: The local reachability density of a data point is defined as the inverse of the average reachability distance of its k-nearest neighbors.\n",
    "\n",
    "4) Compute the LOF score: The LOF score of a data point is defined as the ratio of its local reachability density to the average local reachability density of its k-nearest neighbors.\n",
    "\n",
    "Data points with an LOF score greater than 1 are considered to be outliers or anomalies, while data points with an LOF score less than 1 are considered to be normal. The LOF score reflects the degree to which a data point is an outlier compared to its k-nearest neighbors.\n",
    "\n",
    "The LOF algorithm is effective in detecting anomalies in high-dimensional datasets and can handle datasets with non-uniform density. However, it may be sensitive to the choice of the parameters k and the distance metric, and can be computationally expensive for large datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3abb3b-0b65-40f8-b059-e40cf920df19",
   "metadata": {},
   "source": [
    "7) What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf39c19-0451-4d91-8838-0a1b647863db",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is a tree-based anomaly detection method that isolates anomalies by randomly partitioning the data points into smaller and smaller subsets. The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "1) n_estimators: The number of trees in the forest. Increasing the number of trees can improve the accuracy of the algorithm but also increases the computational cost.\n",
    "\n",
    "2) max_samples: The number of data points to be used for each tree. By default, this parameter is set to \"auto\", which means that the number of samples is equal to the number of data points in the input dataset.\n",
    "\n",
    "3) contamination: The expected percentage of anomalies in the dataset. This parameter is used to set the threshold for anomaly detection. For example, if contamination is set to 0.01, the algorithm will identify the top 1% of data points with the highest anomaly scores as anomalies.\n",
    "\n",
    "4) max_features: The number of features to be used for each split in a tree. By default, this parameter is set to \"auto\", which means that the number of features is equal to the square root of the total number of features.\n",
    "\n",
    "5) random_state: The seed used by the random number generator. This parameter ensures that the results of the algorithm are reproducible.\n",
    "\n",
    "The choice of these parameters depends on the nature and complexity of the dataset being analyzed, as well as the desired level of accuracy and computational cost. Tuning these parameters can significantly improve the performance of the Isolation Forest algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf8462-1c56-47c3-8b59-4c53d4a2096d",
   "metadata": {},
   "source": [
    "8) If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876192bc-fc97-4112-927c-38c206fb5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X,y=make_circles(n_samples=750,factor=0.3,noise=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ffa8f7-5ba1-4068-89cb-42b42d910207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nbrs=NearestNeighbors(n_neighbors=10,radius=0.5,algorithm='auto').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b181652e-6d22-41e6-b65a-8ae82ab5dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_distance = distances[0][-1]\n",
    "avg_distance = distances[0][1:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cad1dd0-f341-45db-9b39-09debeda37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score = (avg_distance / k_distance) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c66269-9171-49a4-b12d-8efe3590c5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.13335558556459393"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9713e-37f9-41ae-a4ab-b5905327b22b",
   "metadata": {},
   "source": [
    "9) Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "907f4004-97e7-4ae3-ab2a-e8a9f0ac7140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>IsolationForest(n_estimators=3000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">IsolationForest</label><div class=\"sk-toggleable__content\"><pre>IsolationForest(n_estimators=3000, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "IsolationForest(n_estimators=3000, random_state=42)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "clf=IsolationForest(n_estimators=3000,contamination='auto',random_state=42)\n",
    "clf.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6090a6-420a-4c27-83a0-37cb3a8cdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_score = avg_path_length_data_point / avg_path_length_trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
