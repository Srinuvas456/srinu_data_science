{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde557e6-6628-421d-9b86-c25a7e55aead",
   "metadata": {},
   "source": [
    "1) What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4f99cc-07f0-45f5-9ade-d2212f50804f",
   "metadata": {},
   "source": [
    "A linear SVM (Support Vector Machine) is a type of classifier that separates data points using a hyperplane. The mathematical formula for a linear SVM can be expressed as:\n",
    "\n",
    "f(x) = sign(w^T x + b)\n",
    "\n",
    "where:\n",
    "\n",
    "x is the input data point \n",
    "\n",
    "w is a vector of weights  that determine the orientation and position of the hyperplane\n",
    "\n",
    "b is a scalar bias term that shifts the hyperplane away from the origin\n",
    "\n",
    "sign() is the sign function, which returns -1 if the argument is negative, 0 if it is zero, and +1 if it is positive.\n",
    "\n",
    "The goal of training a linear SVM is to find the optimal values of w and b that maximize the margin (distance) between the hyperplane and the closest data points from each class. This is achieved by solving an optimization problem that minimizes the norm of w subject to the constraint that all data points are classified correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee62e6c-4e99-48fc-8c3f-6d18f3f83c09",
   "metadata": {},
   "source": [
    "2) What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533e688-164f-4724-877a-de481b286b51",
   "metadata": {},
   "source": [
    "The objective function of a linear SVM  is to find the hyperplane that maximizes the margin between the closest data points from each class. The margin is defined as the distance between the hyperplane and the closest data points. The optimal hyperplane is the one that maximizes this margin, which in turn leads to better generalization performance on unseen data.\n",
    "\n",
    "The objective function of a linear SVM can be expressed as a constrained optimization problem:\n",
    "\n",
    "minimize 1/2 ||w||^2\n",
    "\n",
    "subject to y_i(w^T x_i + b) >= 1 for all i = 1, ..., n\n",
    "\n",
    "where:\n",
    "\n",
    "w is a vector of weights  that determine the orientation and position of the hyperplane\n",
    "\n",
    "b is a scalar bias term that shifts the hyperplane away from the origin\n",
    "\n",
    "x_i is the i-th input data point \n",
    "\n",
    "y_i is the corresponding label (+1 or -1) of the i-th data point\n",
    "\n",
    "n is the number of data points.\n",
    "\n",
    "The first term 1/2 ||w||^2 is a regularization term that penalizes large weights, which helps prevent overfitting. The second term is a constraint that enforces that all data points are classified correctly with a margin of at least 1. This is achieved by ensuring that the product of the predicted label and the actual label is greater than or equal to 1 for all data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba42ff-f95f-4926-a0d3-f756f79a4fe1",
   "metadata": {},
   "source": [
    "3) What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f50c41-9653-405d-8d65-7977b7585d3b",
   "metadata": {},
   "source": [
    "The kernel trick is a technique used in support vector machines (SVM) that allows the algorithm to efficiently find non-linear decision boundaries in high-dimensional feature spaces.\n",
    "\n",
    "In SVM, the goal is to find a hyperplane that separates two classes of data points in a way that maximizes the margin, or the distance between the hyperplane and the closest points of each class. However, when the data is not linearly separable in the original feature space, the SVM algorithm can't find a linear decision boundary that maximizes the margin.\n",
    "\n",
    "The kernel trick solves this problem by transforming the original feature space into a higher-dimensional space, where the data may become linearly separable. This transformation is done by applying a nonlinear function, called a kernel function, to the original feature space. The kernel function calculates the dot product between pairs of data points in the transformed space, without explicitly computing the coordinates of the data points in the higher-dimensional space.\n",
    "\n",
    "The most common kernel functions used in SVM are the linear kernel, polynomial kernel, and radial basis function (RBF) kernel. The RBF kernel is the most commonly used kernel function as it has proven to be effective in many practical applications.\n",
    "\n",
    "The kernel trick enables SVM to effectively learn non-linear decision boundaries in high-dimensional feature spaces, without actually computing the coordinates of the transformed data points. This makes SVM computationally efficient, and allows it to handle complex datasets with large numbers of features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe9683-8d77-45cf-ac9e-f4a51fc00719",
   "metadata": {},
   "source": [
    "4) What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ea9c6-b488-4a36-bb3f-ef83492a8473",
   "metadata": {},
   "source": [
    "In support vector machines (SVM), support vectors are the data points that lie closest to the decision boundary, or hyperplane, and therefore play a critical role in defining the decision boundary and the margin.\n",
    "\n",
    "Support vectors are important because they determine the orientation and position of the decision boundary, as well as the width of the margin. The decision boundary is defined as the hyperplane that separates the data into two classes, with the maximum margin possible, i.e., the distance between the hyperplane and the closest data points of each class.\n",
    "\n",
    "The support vectors are the data points that lie on the margin boundary or within the margin, and their position and orientation determine the hyperplane's location and orientation. In other words, if we move or remove any support vector, the decision boundary would change, and the margin could become smaller or disappear altogether.\n",
    "\n",
    "ex: let's consider a simple binary classification problem where we have two classes of data points in a 2D feature space, and the goal is to find a linear decision boundary that separates the two classes. The following figure shows the two classes of data points and a possible decision boundary (blue line) that separates them\n",
    "\n",
    "In this case, we can see that there are three data points (circled in red) that lie on the margin boundary or within the margin, and they are the support vectors. If we move or remove any of these support vectors, the decision boundary and the margin would change.\n",
    "\n",
    "The SVM algorithm finds the support vectors by solving an optimization problem that aims to maximize the margin between the classes while minimizing the classification error. The support vectors are the data points that are closest to the decision boundary, and they are used to calculate the hyperplane's orientation and position"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5fa63-95d6-41c2-9ead-554053e4fe3c",
   "metadata": {},
   "source": [
    "Let's consider a simple example of a binary classification problem with two features, x1 and x2. We want to separate the two classes using SVM\n",
    "\n",
    "The red points belong to one class, and the blue points belong to the other class. We can see that there is no linear boundary that can separate the two classes.\n",
    "\n",
    "Now, let's explore the different concepts in SVM:\n",
    "\n",
    "Hyperplane:\n",
    "\n",
    "In SVM, a hyperplane is a decision boundary that separates the two classes. For a two-dimensional dataset, the hyperplane is a line. For higher-dimensional datasets, the hyperplane is a hyperplane.\n",
    "\n",
    "ex:\n",
    "\n",
    "SVM_hyperplane\n",
    "\n",
    "The line shown in the figure is the hyperplane. It separates the red and blue classes. The points on the hyperplane are called support vectors.\n",
    "\n",
    "Margin plane:\n",
    "\n",
    "The margin plane is a parallel line to the hyperplane that separates the support vectors of the two classes. The margin is the distance between the hyperplane and the margin plane.\n",
    "\n",
    "ex :\n",
    "\n",
    "SVM_marginplane\n",
    "\n",
    "The blue line is the margin plane, and the red line is the hyperplane. The margin is the distance between the two lines. In this case, the margin is maximized.\n",
    "\n",
    "Hard margin:\n",
    "\n",
    "In a hard margin SVM, the goal is to find a hyperplane that separates the two classes with maximum margin and with no misclassified points. This is only possible when the two classes are linearly separable.\n",
    "\n",
    "ex:\n",
    "\n",
    "SVM_hardmargin\n",
    "\n",
    "The red and blue lines represent the hyperplane and margin plane, respectively. All the points are correctly classified, and the margin is maximized.\n",
    "\n",
    "Soft margin:\n",
    "\n",
    "In a soft margin SVM, the goal is to find a hyperplane that separates the two classes with maximum margin, but allows some misclassified points. This is necessary when the two classes are not linearly separable.\n",
    "\n",
    "ex:\n",
    "\n",
    "SVM_softmargin\n",
    "\n",
    "The red and blue lines represent the hyperplane and margin plane, respectively. In this case, there are some misclassified points (the blue point on the left), but the margin is still maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46f9d15-ea6a-466a-ac55-1694a3dac92f",
   "metadata": {},
   "source": [
    "6) SVM Implementation through Iris dataset.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971b176b-9af9-4625-bd4f-9d3aaddb1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55942622-8158-4276-8a19-85bc15079685",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d843bff8-8e4f-4178-832f-eaf82e7849d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8520115a-68da-40ee-933b-564d08f176a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67270f08-be8b-4179-ae31-9086699b6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7640cae8-8a7d-435f-ba37-8ca440ef2323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2871816-ad62-4d40-bd7c-e7028aa6919b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd5bacf0-0396-43de-903a-0062dfcd416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=0\n",
    "f2=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecdc5b3f-fe50-49cf-86f5-ef3384ce8a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = X_train[:, f1].min() - 1, X_train[:, f1].max() + 1\n",
    "y_min, y_max = X_train[:, f2].min() - 1, X_train[:, f2].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf0de2-eca3-4960-bbac-c10d52a9c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d4ae8a-2d8b-4dbd-a458-589a70235979",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(X_train[:, feature1], X_train[:, feature2], c=y_train, cmap=plt.cm.coolwarm)\n",
    "plt.xlabel(iris.feature_names[feature1])\n",
    "plt.ylabel(iris.feature_names[feature2])\n",
    "plt.title(\"SVM Decision Boundaries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3d2d5f0-a16f-4069-b8df-e958957c1332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (C=0.1): 1.0\n",
      "Accuracy (C=1): 1.0\n",
      "Accuracy (C=10): 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "for C in [0.1, 1, 10]:\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy (C={}): {}\".format(C, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27425806-39f2-47d8-8c7d-f937d1d91438",
   "metadata": {},
   "source": [
    "Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "132b646f-8ae7-4406-9154-103494c72ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM:\n",
    "    def __init__(self, lr=0.01, lambda_reg=0.01, num_epochs=1000):\n",
    "        self.lr = lr\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        y_ = np.where(y <= 0, -1, 1)\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            for i in range(n_samples):\n",
    "                condition = y_[i] * (np.dot(X[i], self.weights) - self.bias) >= 1\n",
    "                if condition:\n",
    "                    self.weights -= self.lr * (2 * self.lambda_reg * self.weights)\n",
    "                else:\n",
    "                    self.weights -= self.lr * (2 * self.lambda_reg * self.weights - np.dot(X[i], y_[i]))\n",
    "                    self.bias -= self.lr * y_[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) - self.bias\n",
    "        return np.sign(linear_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb628f5d-547f-4d28-bd66-9490be56faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a3a30aa-5e70-49d6-84ce-b4ba0a733e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=1000, centers=2, n_features=2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4b70815-1b05-4763-8f80-e355e3d20e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c5183d68-060f-4a9b-8524-d7c3e48032c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (our implementation): 49.67%\n",
      "Accuracy (scikit-learn implementation): 100.00%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf = LinearSVM()\n",
    "clf.fit(X_train, y_train)\n",
    "clf_sklearn = LinearSVC()\n",
    "clf_sklearn.fit(X_train, y_train)\n",
    "accuracy = np.mean(clf.predict(X_test) == y_test)\n",
    "accuracy_sklearn = clf_sklearn.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy (our implementation): {:.2f}%\".format(accuracy*100))\n",
    "print(\"Accuracy (scikit-learn implementation): {:.2f}%\".format(accuracy_sklearn*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630069d7-f221-4175-9b44-cb979f12437a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
