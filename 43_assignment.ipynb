{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a3bcc4-1bcc-4dc0-9952-815caebfddf5",
   "metadata": {},
   "source": [
    "1) What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e2bf0-7329-4aad-999e-554be8ffde81",
   "metadata": {},
   "source": [
    "Min-Max scaling is a common data preprocessing technique used to normalize data values to a specific range. It rescales the original data values to a new range between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a0da9-ec91-4559-b639-2397a6caa81e",
   "metadata": {},
   "source": [
    "where X is the original value, X_min is the minimum value of the dataset, and X_max is the maximum value of the dataset.\n",
    "\n",
    "The application of Min-Max scaling can be illustrated with an example. Suppose we have a dataset of heights (in centimeters) of 20 people"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520f5ca-6a25-427c-995b-073a470db6b0",
   "metadata": {},
   "source": [
    "2) What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ba418-88c9-44dc-ba2a-091b523e22b2",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization, is a feature scaling technique used to transform the values of a dataset into a unit vector, which has a magnitude of 1. The scaling is achieved by dividing each data point by the Euclidean norm of the dataset.\n",
    "\n",
    "The formula for Unit Vector scaling is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9639a-f2c9-4d2d-8497-e3bdc0bc09b1",
   "metadata": {},
   "source": [
    "where X is the original value, ||X|| is the Euclidean norm of the dataset, and X_scaled is the scaled value.\n",
    "\n",
    "The Unit Vector technique is useful in data preprocessing because it can help to normalize the data, which can improve the performance of certain machine learning algorithms, such as those based on gradient descent. Additionally, Unit Vector scaling can help to prevent numerical instability in some optimization algorithms.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Suppose we have a dataset of house prices, with the following values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d59e22-b723-491c-bd42-dd832cb65a12",
   "metadata": {},
   "source": [
    "3) What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5396e9-4c38-43e0-bba5-3a3124da6ecf",
   "metadata": {},
   "source": [
    "PCA, or Principal Component Analysis, is a technique for reducing the dimensionality of a dataset by identifying the underlying patterns in the data and representing them in a lower-dimensional space. The technique works by finding the principal components of the data, which are the linear combinations of the original features that capture the most variance in the data.\n",
    "\n",
    "PCA is used in dimensionality reduction because it can help to reduce the number of features in a dataset while still retaining most of the information. By reducing the number of features, PCA can simplify the analysis of the data and improve the performance of certain machine learning algorithms.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73d10f-3ad3-4856-9c41-7df95bb34ac2",
   "metadata": {},
   "source": [
    "We want to reduce the dimensionality of this dataset to two dimensions so that we can visualize the data in a scatter plot.\n",
    "\n",
    "To apply PCA to this dataset, we first standardize the data so that each feature has zero mean and unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33920176-0491-489a-bcae-87cde277b490",
   "metadata": {},
   "source": [
    "Then, we find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in which the data varies the most, while the eigenvalues represent the amount of variance explained by each eigenvector\n",
    "\n",
    "We then select the top two eigenvectors with the highest eigenvalues, which will form the new two-dimensional feature space. We project the standardized data onto this new feature space to obtain the new two-dimensional dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cee21-b648-4a5d-ac36-5f3c27657cd2",
   "metadata": {},
   "source": [
    "4) What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38796ec6-d542-45ae-a71d-02188cb84931",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used for feature extraction because it is able to identify the most important features in a dataset and create a reduced set of features that capture most of the variation in the data.\n",
    "\n",
    "Feature extraction is the process of transforming raw data into a new set of features that are more meaningful and informative for a particular task or analysis. PCA can be used as a feature extraction technique by identifying the most important features in the data and creating a new set of features that capture most of the variation in the data.\n",
    "\n",
    "Here's an example to illustrate the concept of using PCA for feature extraction:\n",
    "\n",
    "Suppose we have a dataset of images of faces, with each image represented as a vector of pixel values. Each image has 1000 pixels, so the dataset has 1000 features. We want to extract a smaller set of features that can be used for facial recognition.\n",
    "\n",
    "To extract features from this dataset using PCA, we first standardize the data so that each pixel has zero mean and unit variance:\n",
    "\n",
    "Standardize the data for each pixel by subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b116dd-c022-440f-8c6f-397e37882d82",
   "metadata": {},
   "source": [
    "Next, we calculate the covariance matrix of the standardized data. The covariance matrix represents the pairwise covariances between the pixels:\n",
    "\n",
    "Calculate the covariance matrix of the standardized data.\n",
    "Then, we find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in which the data varies the most, while the eigenvalues represent the amount of variance explained by each eigenvector:\n",
    "\n",
    "Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "We then select the top k eigenvectors with the highest eigenvalues, which will form the new set of k features. We project the standardized data onto this new feature space to obtain the new dataset with k features:\n",
    "\n",
    "Project the standardized data onto the new feature space.\n",
    "The new set of k features can then be used for facial recognition or other analyses. The advantage of using PCA for feature extraction is that it can reduce the dimensionality of the data while retaining most of the information, making it easier and faster to analyze the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a71e85a-65a7-42ce-8d9f-34fabb6d236e",
   "metadata": {},
   "source": [
    "5) You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4411b6f6-ea7d-4b77-b268-7a2cde56c884",
   "metadata": {},
   "source": [
    "To use Min-Max scaling to preprocess the data for a food delivery recommendation system, we would follow these steps:\n",
    "\n",
    "1) Identify the features that need to be scaled: In this case, the features that need to be scaled are price, rating, and delivery time.\n",
    "\n",
    "2) Compute the minimum and maximum values for each feature: We would compute the minimum and maximum values for each feature in the dataset. For example, the minimum and maximum prices, ratings, and delivery times across all restaurants in the dataset.\n",
    "\n",
    "3) Apply the Min-Max scaling formula to each feature: We would apply the Min-Max scaling formula to each feature, which scales the values in the feature to a range of 0 to 1. The formula is:\n",
    "\n",
    "4) scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "For example, to scale the price feature, we would compute the scaled value for each restaurant in the dataset using the formula above.\n",
    "\n",
    "Replace the original values with the scaled values: After computing the scaled values for each feature, we would replace the original values in the dataset with the scaled values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7e7f56-5de2-46f9-b857-f6fd303bbba6",
   "metadata": {},
   "source": [
    "6) You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c567a-aad8-4f71-a3f9-1bac2d35e9af",
   "metadata": {},
   "source": [
    "To use PCA (Principal Component Analysis) to reduce the dimensionality of the stock price dataset, we would follow these steps:\n",
    "\n",
    "1) Standardize the data: We would standardize the data by subtracting the mean and dividing by the standard deviation for each feature. This is important because PCA is sensitive to the scale of the features, and standardizing the data ensures that each feature has equal weight in the analysis.\n",
    "\n",
    "2) Compute the covariance matrix: We would compute the covariance matrix of the standardized data. The covariance matrix represents the pairwise covariances between the features and is used to calculate the principal components.\n",
    "\n",
    "3) Compute the eigenvectors and eigenvalues: We would compute the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the directions in which the data varies the most, while the eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "4) Select the principal components: We would select the top k eigenvectors with the highest eigenvalues, which will form the new set of k features. The number of principal components to select depends on the amount of variance we want to retain in the data. For example, we might choose to retain 95% of the variance in the data.\n",
    "\n",
    "5) Project the data onto the new feature space: We would project the standardized data onto the new feature space formed by the top k eigenvectors. This will create a new dataset with reduced dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2dbac4-fbe2-4d3d-ac5c-e607477f6b4e",
   "metadata": {},
   "source": [
    "7) For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfcb77e-fd49-432f-a4d4-9158df86e8b7",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the values [1, 5, 10, 15, 20] to a range of -1 to 1, we would follow these steps:\n",
    "\n",
    "1) Compute the minimum and maximum values in the dataset:\n",
    "\n",
    "min = 1\n",
    "max = 20\n",
    "2) Compute the range of the dataset:\n",
    "\n",
    "range = max - min = 20 - 1 = 19\n",
    "3) Compute the scaling factor:\n",
    "\n",
    "scaling factor = 2 / range = 2 / 19 = 0.10526315789\n",
    "4) Compute the shift factor:\n",
    "\n",
    "shift factor = -1 - (scaling factor * min) = -1 - (0.10526315789 * 1) = -1.10526315789\n",
    "5) Apply the Min-Max scaling formula to each value in the dataset:\n",
    "\n",
    "scaled_value = (original_value * scaling factor) + shift factor\n",
    "Applying this formula to each value in the dataset, we get:\n",
    "\n",
    "Scaled value of 1 = (-1.10526315789)\n",
    "Scaled value of 5 = (-0.57894736842)\n",
    "Scaled value of 10 = (0.0)\n",
    "Scaled value of 15 = (0.57894736842)\n",
    "Scaled value of 20 = (1.10526315789)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300bf04-e4b3-4fcb-8aae-ee9967d97430",
   "metadata": {},
   "source": [
    "Therefore, after applying Min-Max scaling, the dataset would be transformed to [-1.105, -0.579, 0.000, 0.579, 1.105] in the range of -1 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10524ad-e1b4-4357-a9da-3b97102c3b45",
   "metadata": {},
   "source": [
    "8) For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd2117-a847-45fa-8448-12c5d0b93c20",
   "metadata": {},
   "source": [
    "The number of principal components to retain during PCA depends on the amount of variance we want to retain in the data. In order to determine how many principal components to choose, we can perform PCA on the dataset and plot the explained variance ratio for each principal component.\n",
    "\n",
    "The explained variance ratio represents the proportion of variance in the original data that is explained by each principal component. By plotting the explained variance ratio for each component, we can determine how many components we need to retain in order to retain a desired amount of variance.\n",
    "\n",
    "Assuming that the dataset has been properly preprocessed and scaled, we would follow these steps to perform feature extraction using PCA:\n",
    "\n",
    "1) Compute the covariance matrix of the dataset.\n",
    "\n",
    "2) Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "3) Calculate the explained variance ratio for each principal component by dividing each eigenvalue by the sum of all eigenvalues.\n",
    "\n",
    "4) Plot the explained variance ratio for each principal component and determine the number of components to retain based on the desired amount of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a5e2f-6236-480a-8072-408af19f2df0",
   "metadata": {},
   "source": [
    "The number of principal components to retain would depend on the desired amount of variance that we want to retain. For example, if we want to retain 95% of the variance in the data, we would select the smallest number of principal components that explain at least 95% of the variance.\n",
    "\n",
    "Therefore, without knowing the amount of variance that needs to be retained, it is difficult to determine how many principal components to choose for the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b6b7c0-8254-4a44-a772-d149f4849569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
