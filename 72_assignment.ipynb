{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b4c761-32aa-4394-a549-38d9c817b112",
   "metadata": {},
   "source": [
    "1) What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f9359-36a8-4788-abc5-10e0858a6833",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a popular machine learning technique that is used for building predictive models. It is a type of boosting algorithm that builds a strong regression model by combining the predictions of multiple weak regression models.\n",
    "\n",
    "In Gradient Boosting Regression, a weak regression model is first trained on the data. The residuals (the difference between the actual and predicted values) of this model are then computed, and the next weak regression model is trained on these residuals. The process is repeated until the specified number of weak models is reached or a predefined stopping criterion is met.\n",
    "\n",
    "In each iteration, the new weak model is trained to predict the negative gradient of the loss function with respect to the predicted values of the previous models. This means that the new model is trained to predict the direction and magnitude of the error in the previous models, so that the errors can be corrected in the final model.\n",
    "\n",
    "The predictions of the weak models are combined to form the final prediction by summing the predicted values of each model, weighted by a factor that is determined during the training process. The final model is thus a weighted sum of the predictions of the weak models, and is able to capture complex nonlinear relationships between the input variables and the target variable.\n",
    "\n",
    "Gradient Boosting Regression is widely used in applications such as financial forecasting, marketing analytics, and computer vision, where accurate predictions are critical. It is a powerful technique that can handle a wide range of data types and can be customized to fit a variety of modeling scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427402ab-f3df-4b9f-81ab-121e5bd01b08",
   "metadata": {},
   "source": [
    "2) Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "199f972a-36a5-4ea8-8849-410042e22d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "diabetes=load_diabetes()\n",
    "X,y=diabetes.data,diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5819f0f8-9a07-4f85-bb82-db2032551d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((442, 10), (442,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "058b6517-951b-4869-9aa2-c8545328d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8b32982-3154-46ee-b225-cc6bf65fff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d3ca089-291e-4f06-a0e0-69a9c5a8f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, max_depth=3, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.trees = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.f0 = np.mean(y)\n",
    "        for i in range(self.n_estimators):\n",
    "            y_pred = self.predict(X)\n",
    "            residuals = y - y_pred\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = np.full(X.shape[0], self.f0)\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4975f94-26f9-4bfb-8b1e-7b94e60238f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2885.4675741268447\n",
      "R-squared: 0.4553822330712347\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "model = GradientBoostingRegressor(n_estimators=100, max_depth=3, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('R-squared:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2286235c-dc40-44b7-8d0a-7073522b95b8",
   "metadata": {},
   "source": [
    "3) Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e093824-43b5-4daf-8924-3194980175fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
      "Best Score: 3428.5195207983184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.5]\n",
    "}\n",
    "model = GradientBoostingRegressor()\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best Hyperparameters:', grid_search.best_params_)\n",
    "print('Best Score:', -grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee50171-8906-4c36-b5b7-dafa48f53a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3071.0637663987136\n",
      "R-squared: 0.4203518675623885\n"
     ]
    }
   ],
   "source": [
    "best_model = GradientBoostingRegressor(n_estimators=150, max_depth=4, learning_rate=0.1)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('R-squared:', r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2c948-c159-4c2a-bd3c-9c3767f72ff9",
   "metadata": {},
   "source": [
    "4) What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff34fecf-e47d-4843-9e4a-af8e678f1e93",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner is a simple model or algorithm that performs slightly better than random guessing. Typically, a decision tree with very low depth is used as a weak learner, which is also called a decision stump. The weak learner is trained on the residuals of the previous model in the boosting process, and its predictions are combined with the predictions of the previous model to improve the overall performance. Since each weak learner is only slightly better than random guessing, multiple weak learners are combined to create a strong learner that can accurately predict the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1a9ae-12e2-4cba-be9a-fa4022e154c4",
   "metadata": {},
   "source": [
    "5) What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b0703-527e-450b-9133-33cc3e565b07",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to iteratively improve the predictions of a model by adding new weak learners to the existing model. The algorithm starts with a simple model that predicts the mean value of the target variable for all data points, and then builds new models to predict the residuals (differences between the actual target values and the predictions of the previous model). The residuals are used as the new target variable for the next model, and the process is repeated until the desired level of accuracy is achieved.\n",
    "\n",
    "In each iteration, the new weak learner is trained to minimize the loss function of the residuals, which is often the mean squared error or log loss. The weak learner is usually a decision tree with very low depth, which is fast to train and less prone to overfitting. The predictions of the new weak learner are combined with the predictions of the previous model using a small learning rate, which controls the contribution of the new model to the overall prediction. This process is repeated for a fixed number of iterations, or until the desired level of accuracy is achieved.\n",
    "\n",
    "The intuition behind this algorithm is that each new weak learner is trained to correct the mistakes of the previous model, which leads to a gradual improvement in the overall prediction accuracy. By combining multiple weak learners, the algorithm creates a strong model that can accurately predict the target variable, even in the presence of noisy or complex data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4013c-9d8e-4e0d-add3-50ad2f17d146",
   "metadata": {},
   "source": [
    "6) How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0d0f6-e6c9-435a-bb42-bdd7d8001c34",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners by iteratively adding new models to the existing ensemble, each of which is trained to correct the errors of the previous models. The process involves the following steps:\n",
    "\n",
    "1) Initialize the ensemble: The algorithm starts with an initial prediction, usually the mean value of the target variable for all data points.\n",
    "\n",
    "2) Calculate the residual errors: The algorithm calculates the difference between the actual target values and the predictions of the current ensemble. These residual errors become the new target variable for the next model.\n",
    "\n",
    "3) Train a weak learner on the residuals: The algorithm trains a new weak learner (usually a decision tree with low depth) to predict the residual errors. The weak learner is trained to minimize the loss function of the residuals, such as mean squared error or log loss.\n",
    "\n",
    "4) Update the ensemble: The predictions of the new weak learner are combined with the predictions of the previous ensemble using a small learning rate. This controls the contribution of the new model to the overall prediction, and helps prevent overfitting.\n",
    "\n",
    "5) Repeat: Steps 2-4 are repeated until the desired level of accuracy is achieved, or until a stopping criterion is met.\n",
    "\n",
    "By combining multiple weak learners, the Gradient Boosting algorithm creates a strong ensemble that can accurately predict the target variable. Each new weak learner is trained to correct the errors of the previous models, which leads to a gradual improvement in the overall prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c951aecd-b96b-4c00-a194-78a889b0daf0",
   "metadata": {},
   "source": [
    "7) What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1b8a7-2de3-4484-8709-99a840f5976b",
   "metadata": {},
   "source": [
    "The mathematical intuition of the Gradient Boosting algorithm involves the following steps:\n",
    "\n",
    "1) Define the loss function: The first step is to define a loss function that measures the difference between the predicted values and the actual values. For example, in regression problems, the mean squared error (MSE) is commonly used as the loss function.\n",
    "\n",
    "2) Initialize the model: The algorithm starts with an initial prediction, usually the mean value of the target variable for all data points.\n",
    "\n",
    "3) Calculate the negative gradient of the loss function: The negative gradient of the loss function with respect to the predicted values is calculated. This represents the direction of steepest descent of the loss function, and indicates how the predicted values should be adjusted to reduce the loss.\n",
    "\n",
    "4) Train a weak learner on the negative gradient: A new weak learner (usually a decision tree with low depth) is trained to predict the negative gradient of the loss function. The weak learner is trained to minimize the loss function of the negative gradients, using the data points and their corresponding negative gradients as inputs.\n",
    "\n",
    "5) Update the model: The predictions of the new weak learner are added to the predictions of the previous model, using a small learning rate to control the contribution of the new model to the overall prediction.\n",
    "\n",
    "6) Repeat: Steps 3-5 are repeated until the desired level of accuracy is achieved, or until a stopping criterion is met.\n",
    "\n",
    "The key idea behind the Gradient Boosting algorithm is to iteratively add new models to the ensemble that predict the negative gradients of the loss function, and combine their predictions to improve the overall prediction accuracy. By minimizing the loss function of the negative gradients, the algorithm effectively minimizes the original loss function, leading to better predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1d6385-adf2-481d-8ab0-535e1287c12f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
