{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e103e707-4b3f-4bae-b9c1-5cc92c5fb3c0",
   "metadata": {},
   "source": [
    "1) Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb57dcc-d0c5-4453-81c9-2f7e12567f4c",
   "metadata": {},
   "source": [
    "Simple linear regression is a statistical technique that involves finding the best-fit line to model the linear relationship between two variables: one independent variable (also known as predictor or explanatory variable) and one dependent variable (also known as response variable). It aims to estimate the value of the dependent variable based on the value of the independent variable. For example, we might use simple linear regression to model the relationship between a person's weight (independent variable) and their height (dependent variable), to predict someone's height given their weight.\n",
    "\n",
    "On the other hand, multiple linear regression is a statistical technique that involves finding the best-fit line to model the linear relationship between one dependent variable and multiple independent variables. It aims to estimate the value of the dependent variable based on the values of multiple independent variables. For example, we might use multiple linear regression to model the relationship between a person's salary (dependent variable) and their age, education level, and work experience (independent variables), to predict someone's salary given their age, education level, and work experience.\n",
    "\n",
    "ex:\n",
    "\n",
    "Simple linear regression: Let's say we want to predict a person's score on a math test based on the number of hours they study. We collect data from a sample of students, record their hours of study and their scores on the test, and plot the data on a scatterplot. We can then use simple linear regression to find the line that best fits the data, which will allow us to make predictions about a person's score based on their hours of study.\n",
    "\n",
    "Multiple linear regression: Let's say we want to predict the price of a house based on its size, number of bedrooms, and location. We collect data on a sample of houses, record their size, number of bedrooms, location, and price, and use multiple linear regression to find the best-fit line that relates these variables to the house's price. We can then use this line to predict the price of a house based on its size, number of bedrooms, and location.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d7f67-29eb-47ca-833c-72f93296f6f6",
   "metadata": {},
   "source": [
    "2) Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5305d6fd-f3bd-4d93-9a53-150186332476",
   "metadata": {},
   "source": [
    "Linear regression is a statistical technique that relies on several assumptions to produce accurate results. These assumptions are important to consider because they affect the validity and reliability of the regression model. The main assumptions of linear regression are:\n",
    "\n",
    "1) Linearity: The relationship between the independent variable(s) and the dependent variable should be linear. This means that the slope of the relationship should remain constant throughout the range of the data.\n",
    "\n",
    "2) Independence: The observations should be independent of each other. This means that the value of the dependent variable for one observation should not be influenced by the value of the dependent variable for another observation.\n",
    "\n",
    "3) Homoscedasticity: The variance of the residuals (the differences between the observed values and the predicted values) should be constant across all levels of the independent variable(s).\n",
    "\n",
    "4) Normality: The residuals should be normally distributed. This means that the distribution of the residuals should follow a bell-shaped curve.\n",
    "\n",
    "5) No multicollinearity: If multiple independent variables are used in the model, they should not be highly correlated with each other. This means that they should not be measuring the same thing.\n",
    "\n",
    "To check whether these assumptions hold in a given dataset, several methods can be used:\n",
    "\n",
    "1) Plotting the data: A scatterplot can be used to visually examine the linearity of the relationship between the independent and dependent variables.\n",
    "\n",
    "2) Residual plots: A scatterplot of the residuals can be used to check for homoscedasticity and normality.\n",
    "\n",
    "3) Correlation matrix: A correlation matrix can be used to check for multicollinearity.\n",
    "\n",
    "4) Statistical tests: Statistical tests such as the Shapiro-Wilk test for normality and the Breusch-Pagan test for homoscedasticity can be used to formally test these assumptions.\n",
    "\n",
    "If the assumptions are not met, several techniques can be used to address the issue. For example, transformations can be applied to the variables to achieve linearity, outliers can be removed to achieve independence, and non-linear models can be used to address non-normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fea78-f5e0-49c5-aeae-8cf1542ee81e",
   "metadata": {},
   "source": [
    "3) How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8416e-d5de-4226-a0f1-b13fbc51d3f4",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept are two important coefficients that can help interpret the relationship between the independent variable(s) and the dependent variable. The slope represents the change in the dependent variable for a one-unit increase in the independent variable, while the intercept represents the expected value of the dependent variable when the independent variable is zero.\n",
    "\n",
    "For example, consider a linear regression model that predicts a person's monthly electricity bill based on the number of people living in their house. The model might have the following equation:\n",
    "\n",
    "Electricity bill = 50 + 30 x Number of people\n",
    "\n",
    "In this model, the intercept is 50, which represents the expected monthly electricity bill for a household with zero people . The slope is 30, which means that for each additional person in the household, the monthly electricity bill is expected to increase by $30.\n",
    "\n",
    "Therefore, we can interpret the coefficients as follows: the intercept represents the baseline level of the dependent variable when the independent variable is zero, while the slope represents the change in the dependent variable for a one-unit increase in the independent variable.\n",
    "\n",
    "Another real-world scenario where a linear regression model could be applied is in predicting the price of a car based on its age. Suppose we have a dataset that includes the age of a car and its price, and we want to build a linear regression model to predict the price of a car based on its age. The model might have the following equation:\n",
    "\n",
    "Car price = 20,000 - 2,500 x Age\n",
    "\n",
    "In this model, the intercept is 20,000, which represents the expected price of a car when it is brand new . The slope is -2,500, which means that for each additional year that the car ages, its price is expected to decrease by $2,500.\n",
    "\n",
    "Therefore, we can interpret the coefficients as follows: the intercept represents the price of a car when it is brand new, while the slope represents the decrease in the price of the car for a one-year increase in its age.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee78621-9ccc-46c9-a275-47b36e264d7a",
   "metadata": {},
   "source": [
    "4) Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0617416-c311-47ef-8a06-6bc2de513fea",
   "metadata": {},
   "source": [
    "Gradient descent is a numerical optimization algorithm that is commonly used in machine learning to find the optimal values of the parameters in a model that minimize a cost function.\n",
    "\n",
    "The basic idea of gradient descent is to iteratively adjust the values of the model parameters in the direction of the negative gradient of the cost function until the algorithm converges to the minimum value of the cost function. The negative gradient is used because the goal is to minimize the cost function, and moving in the direction of the negative gradient will move the algorithm towards the minimum.\n",
    "\n",
    "In more technical terms, gradient descent calculates the gradient of the cost function with respect to each parameter in the model. It then updates the values of the parameters by subtracting a fraction of the gradient from the current values. This fraction is called the learning rate and determines how large of a step the algorithm takes in each iteration.\n",
    "\n",
    "There are two main types of gradient descent: batch gradient descent and stochastic gradient descent. Batch gradient descent updates the parameters using the gradient of the cost function calculated over the entire training set, while stochastic gradient descent updates the parameters using the gradient of the cost function calculated for each individual data point in the training set.\n",
    "\n",
    "Gradient descent is used in machine learning to train a wide range of models, including linear regression, logistic regression, neural networks, and many others. By iteratively adjusting the model parameters in the direction of the negative gradient of the cost function, gradient descent allows the model to learn the relationship between the input data and the output labels, ultimately leading to better predictive performance on unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df109bcb-4a3b-4b79-ad69-d65e308eb9d3",
   "metadata": {},
   "source": [
    "5) Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d25bd-fe23-4b8f-be4a-fe7ad8772557",
   "metadata": {},
   "source": [
    "In multiple linear regression, there are multiple independent variables that are used to predict a single dependent variable. The multiple linear regression model takes the form:\n",
    "\n",
    "y = b0 + b1x1 + b2x2 + ... + bnxn + e\n",
    "\n",
    "where y is the dependent variable, x1, x2, ..., xn are the independent variables, b0 is the intercept, and b1, b2, ..., bn are the coefficients that represent the change in y for a one-unit change in each independent variable, holding all other variables constant. e is the error term, which captures the random variation in y that is not explained by the independent variables.\n",
    "\n",
    "The main difference between multiple linear regression and simple linear regression is that in simple linear regression, there is only one independent variable that is used to predict the dependent variable, while in multiple linear regression, there are multiple independent variables. In simple linear regression, the model takes the form:\n",
    "\n",
    "y = b0 + b1x + e\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, b0 is the intercept, b1 is the coefficient that represents the change in y for a one-unit change in x, and e is the error term.\n",
    "\n",
    "Multiple linear regression allows us to model more complex relationships between the dependent variable and the independent variables, and to control for the effects of multiple variables simultaneously. However, it also requires more data and assumptions, and can be more difficult to interpret than simple linear regression.\n",
    "\n",
    "In summary, multiple linear regression is a statistical model that uses multiple independent variables to predict a single dependent variable, while simple linear regression uses only one independent variable to predict the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02204acb-c5ec-4ef5-9572-1a273c83ed7d",
   "metadata": {},
   "source": [
    "6) Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4528861-669c-41a3-82e6-0b994e951596",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation in which two or more independent variables in a multiple linear regression model are highly correlated with each other. This can cause problems in the regression analysis because it makes it difficult to isolate the effect of each independent variable on the dependent variable.\n",
    "\n",
    "When multicollinearity occurs, the estimated coefficients in the regression model may be unstable or imprecise, and it may be difficult to determine which independent variables are truly contributing to the prediction of the dependent variable.\n",
    "\n",
    "One way to detect multicollinearity is to look at the correlation matrix between the independent variables. Correlations that are close to 1 or -1 indicate high multicollinearity. Another way is to look at the variance inflation factor (VIF) for each independent variable. The VIF measures how much the variance of the estimated coefficient for each independent variable is increased due to multicollinearity with the other independent variables. VIF values greater than 5 or 10 are often considered to indicate high levels of multicollinearity.\n",
    "\n",
    "To address multicollinearity, there are several techniques that can be used. One approach is to remove one or more of the highly correlated independent variables from the model. Another approach is to combine the highly correlated variables into a single variable, using techniques such as principal component analysis or factor analysis.\n",
    "\n",
    "Regularization techniques such as ridge regression and lasso regression can also be used to address multicollinearity by adding a penalty term to the regression coefficients, which helps to shrink the coefficients of highly correlated variables towards zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f5527e-6fe8-4622-933a-c3334aaf9a07",
   "metadata": {},
   "source": [
    "7) Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d09fc6-6343-49b8-b2ad-ff5dc86bcd35",
   "metadata": {},
   "source": [
    "In polynomial regression, we use a polynomial function to model the relationship between the independent variable and the dependent variable. The polynomial function takes the form:\n",
    "\n",
    "y = b0 + b1x + b2x^2 + ... + bnx^n + e\n",
    "\n",
    "where y is the dependent variable, x is the independent variable, b0, b1, b2, ..., bn are the coefficients, and e is the error term.\n",
    "\n",
    "The main difference between polynomial regression and linear regression is that in polynomial regression, we use a polynomial function of degree n to fit the data, whereas in linear regression, we use a linear function to fit the data.\n",
    "\n",
    "In linear regression, the relationship between the independent variable and the dependent variable is assumed to be linear, which means that the change in the dependent variable is proportional to the change in the independent variable. However, in many real-world scenarios, the relationship between the independent variable and the dependent variable is not linear, and a polynomial function may better capture the non-linear relationship.\n",
    "\n",
    "Polynomial regression can be used to fit non-linear patterns in the data, and can also be used to model interactions between the independent variables. However, it is important to note that polynomial regression can be prone to overfitting, which occurs when the model fits the noise in the data rather than the underlying pattern. To avoid overfitting, it is important to use techniques such as cross-validation and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a105f23-dc45-4bd7-91d2-bf55c49cc48a",
   "metadata": {},
   "source": [
    "8) What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584515cc-3a69-4451-8ecd-aaa7f805561f",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Polynomial regression can fit a wider range of functional relationships between the independent and dependent variables compared to linear regression.\n",
    "It can capture non-linear patterns that are not possible with linear regression.\n",
    "It can model interactions between the independent variables.\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Polynomial regression can be prone to overfitting if the degree of the polynomial is too high.\n",
    "It can be computationally intensive and may require more resources than linear regression.\n",
    "It can be difficult to interpret the coefficients of a polynomial regression model.\n",
    "In situations where the relationship between the independent and dependent variables is non-linear, polynomial regression may be a better choice than linear regression. For example, if there is a U-shaped or inverted U-shaped relationship between the independent and dependent variables, a quadratic or cubic polynomial function may better capture the relationship. Polynomial regression may also be useful in modeling interactions between the independent variables. However, it is important to be cautious of overfitting and to properly validate the model using techniques such as cross-validation.\n",
    "\n",
    "In general, the choice between linear and polynomial regression depends on the nature of the data and the research question. Linear regression is appropriate when the relationship between the independent and dependent variables is linear, while polynomial regression is appropriate when the relationship is non-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62460e79-bed7-4176-99cc-f55366f91606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
