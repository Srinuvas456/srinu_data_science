{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef68087f-0fbf-4e9f-99fc-96b46227118f",
   "metadata": {},
   "source": [
    "1) What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35fe650-2006-4265-9b58-71c27daaf63b",
   "metadata": {},
   "source": [
    "Missing values refer to the absence of a particular value in a dataset that should have been present. It can occur for various reasons, such as data entry errors, data corruption, or data that was not collected for a particular record.\n",
    "\n",
    "Handling missing values is essential because missing data can impact the accuracy of statistical analyses and machine learning models. Missing data can also lead to biased results, misinterpretations, and incorrect predictions. Therefore, it is crucial to handle missing values effectively before using the data for any analysis or modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2a122d-de7e-4dd0-b4c2-266c1a5bb6b5",
   "metadata": {},
   "source": [
    "1) Decision trees : it is not affected by missing values because they partition the data based on the available attributes, and missing values are simply treated as another category\n",
    "2) Random Forests : It is not impacted by missing values because they construct multiple decision trees, and each tree can handle the missing values differently\n",
    "3) Gradient Boosting : It is a tree-based algorithm that can handle missing values effectively by treating them as a separate category\n",
    "4) Naive Bayes : It is not affected by missing values because it calculates probabilities based on the available attributes only\n",
    "5) K-nearest neighbors : It is another algorithm that is not affected by missing values because it uses the available attributes to identify the nearest neighbors without explicitly considering missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1bbf41-af17-4da3-b513-4bfe4bf9a16d",
   "metadata": {},
   "source": [
    "2) List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc8f2f-c7ed-4fba-a14e-e174d592cdaf",
   "metadata": {},
   "source": [
    "1) Dropping missing Values : One approach to handle missing data is to drop the rows or columns that contain missing values. This technique can be effective if the missing values are randomly distributed in the dataset and do not constitute a significant proportion of the data. However, if the missing values are not randomly distributed, this technique can lead to biased results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca39e6ad-3004-4f1b-b5ae-5a898c5442be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C\n",
       "2  3.0  4.0   9.0\n",
       "4  5.0  6.0  11.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'A': [1, 2, 3, None, 5], 'B': [2, None, 4, 5, 6], 'C': [None, 8, 9, 10, 11]}\n",
    "df = pd.DataFrame(data)\n",
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c11dac52-14be-4730-a119-b8068b54a852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d578a-0efc-446d-886c-f4a39e5acbe9",
   "metadata": {},
   "source": [
    "2) Imputting missing value : Another approach to handle missing data is to impute or fill in the missing values with a value derived from the existing data. This technique can be effective if the missing values are not too many and the imputed values are reasonable. There are several methods for imputing missing values, such as mean imputation, median imputation, and mode imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e44711c-c32e-44d1-a761-622aca8f4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'A': [1, 2, 3, None, 5], 'B': [2, None, 4, 5, 6], 'C': [None, 8, 9, 10, 11]}\n",
    "df = pd.DataFrame(data)\n",
    "mean = df.mean()\n",
    "df = df.fillna(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8836443-3751-48b2-b850-ed2219c4143c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.00</td>\n",
       "      <td>4.25</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.75</td>\n",
       "      <td>5.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A     B     C\n",
       "0  1.00  2.00   9.5\n",
       "1  2.00  4.25   8.0\n",
       "2  3.00  4.00   9.0\n",
       "3  2.75  5.00  10.0\n",
       "4  5.00  6.00  11.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88b93c31-c574-4b19-888b-2cbbdff46574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = {'A': [1, 2, 3, None, 5], 'B': [2, None, 4, 5, 6], 'C': [None, 8, 9, 10, 11]}\n",
    "df = pd.DataFrame(data)\n",
    "median = df.median()\n",
    "df = df.fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38aa5676-89a2-4b23-8790-d43c5e4075a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C\n",
       "0  1.0  2.0   9.5\n",
       "1  2.0  4.5   8.0\n",
       "2  3.0  4.0   9.0\n",
       "3  2.5  5.0  10.0\n",
       "4  5.0  6.0  11.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01012f4b-e87b-4a07-9300-53c8a3ae1097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B     C\n",
       "0  1.0  2.0   8.0\n",
       "1  2.0  2.0   8.0\n",
       "2  3.0  4.0   9.0\n",
       "3  1.0  5.0  10.0\n",
       "4  5.0  6.0  11.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'A': [1, 2, 3, None, 5], 'B': [2, None, 4, 5, 6], 'C': [None, 8, 9, 10, 11]}\n",
    "df = pd.DataFrame(data)\n",
    "mode = df.mode().iloc[0]\n",
    "df= df.fillna(mode)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec63d4d-3bae-45f1-b413-e6170b376a53",
   "metadata": {},
   "source": [
    "3) Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d8fa2b-55cf-4636-b212-dacc22ff4101",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of the target variable or outcome variable in a dataset is not equal or close to equal. In other words, one class has a significantly smaller number of instances than the other class(es). For example, in a binary classification problem, if 90% of the data belongs to class A and only 10% of the data belongs to class B, then the data is imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a0124-6bb4-4b17-bbc1-92d13500644c",
   "metadata": {},
   "source": [
    "1) Biased model performance : Machine learning algorithms are designed to optimize the overall accuracy of the model, which means they will prioritize predicting the majority class correctly and may ignore the minority class. As a result, the model will have a high accuracy score but may perform poorly in predicting the minority class\n",
    "2) Overfitting : Overfitting occurs when the model is too complex and fits the training data too closely, leading to poor generalization on the unseen test data. In the case of imbalanced data, overfitting can occur because the model can predict the minority class perfectly in the training data but fail to generalize to the test data\n",
    "3) Poor decision-making :  In some applications, such as fraud detection or disease diagnosis, the cost of misclassification for the minority class is much higher than the majority class. In such cases, if the model is not trained to handle imbalanced data, it can lead to poor decision-making and high costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00895d-0ddd-4456-a0e5-cce203319cd1",
   "metadata": {},
   "source": [
    "4) What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc59a75-93a3-44d9-a7e6-23a81ad660e8",
   "metadata": {},
   "source": [
    "Up-sampling and down-sampling are two common techniques used to handle imbalanced data. Both techniques are used to balance the distribution of the target variable in the dataset.\n",
    "\n",
    "Up-sampling involves increasing the number of instances in the minority class to match the number of instances in the majority class. This can be done by either replicating the existing instances or generating synthetic instances using techniques like SMOTE (Synthetic Minority Over-sampling Technique).\n",
    "\n",
    "Down-sampling, on the other hand, involves reducing the number of instances in the majority class to match the number of instances in the minority class. This can be done by randomly removing instances from the majority class or using techniques like Tomek Links or Cluster Centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec6f265-2869-42e1-bd15-42ada73ce796",
   "metadata": {},
   "source": [
    "An example of when up-sampling and down-sampling are required is in credit card fraud detection. In this case, the majority of credit card transactions are legitimate, and only a small percentage of transactions are fraudulent. If we use the raw data to train a model, it may have a high accuracy score, but it will not be able to identify the fraudulent transactions accurately.\n",
    "\n",
    "In this case, we can use up-sampling or down-sampling to balance the distribution of the target variable. If we up-sample the minority class, we can generate synthetic instances of fraudulent transactions and balance the dataset. On the other hand, if we down-sample the majority class, we can remove some of the legitimate transactions and balance the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fee24f7-68bc-4d66-9c26-2f5c6f117323",
   "metadata": {},
   "source": [
    "5) What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5674b0a-af90-4c26-8695-609fc85ce97c",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by generating new instances through transformations of the original data. The goal of data augmentation is to create new samples that are similar to the original data but different enough to be considered unique.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique used to handle imbalanced data. It is specifically designed to generate synthetic samples for the minority class in a binary classification problem where the distribution of the target variable is imbalanced.\n",
    "\n",
    "The SMOTE algorithm works by identifying the minority class instances and selecting a random instance. It then selects one or more of its nearest neighbors and generates synthetic samples by interpolating between the selected instance and its neighbors. The synthetic samples are generated in a way that preserves the feature space distribution of the minority class and does not introduce new information into the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a23600-2ab2-4683-be34-f5daaa90f695",
   "metadata": {},
   "source": [
    "example, suppose we have a dataset with 10% of the data belonging to the minority class and 90% belonging to the majority class. If we apply SMOTE, it will generate new synthetic instances for the minority class to increase its representation in the dataset. The new instances will be created by interpolating between the minority class instances, ensuring that the new instances are similar to the original data but different enough to be considered unique.\n",
    "\n",
    "SMOTE is a popular technique because it is simple to implement and can be used with a wide range of machine learning algorithms. However, it should be used with caution as generating too many synthetic samples can lead to overfitting and poor generalization performance. Additionally, SMOTE may not be effective in all cases and should be evaluated carefully before use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f7a3cc-3370-4800-8ac8-9ef97c10470e",
   "metadata": {},
   "source": [
    "6) What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef95ad16-887d-47b4-be2e-73812cdb6d63",
   "metadata": {},
   "source": [
    "Outliers are data points that deviate significantly from the majority of the data points in a dataset. They are data points that are unusually high or low and may be the result of errors in data collection, measurement errors, or natural variation in the data. Outliers can have a significant impact on statistical analysis and machine learning models, as they can skew the results and reduce the accuracy of the model\n",
    "\n",
    "1) Skewed outliers : It is significantly affect the results of statistical analysis, such as mean, median, and standard deviation. The presence of outliers can skew these measures, leading to inaccurate conclusions and decision-making\n",
    "2) Reduced accuracy of machine learning : It is negatively impact the performance of machine learning models, leading to overfitting or underfitting. Outliers can cause the model to learn patterns that are not representative of the data, leading to poor generalization on unseen data\n",
    "3) Improved data quality : Removing or handling outliers can improve the overall quality of the data, making it more reliable and representative of the underlying phenomenon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cb39fc-3d47-4649-9181-101b35bd89d4",
   "metadata": {},
   "source": [
    "1) Removing Outliers : This involves identifying outliers and removing them from the dataset. However, this approach should be used with caution, as it can lead to a loss of information and bias in the dataset\n",
    "2) Transforming the data : This involves transforming the data to reduce the impact of outliers. Common transformation techniques include logarithmic transformation, Box-Cox transformation, and Winsorization\n",
    "3) Using robot statistical methods : This involves using statistical methods that are less sensitive to outliers, such as the median instead of the mean, or using non-parametric methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890c8d1-6a75-48c7-ade9-734ce2dc339d",
   "metadata": {},
   "source": [
    "7) You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574d18d4-1ef1-4657-ad92-c77b97009ff2",
   "metadata": {},
   "source": [
    "1) Delete missing data : This involves removing any rows or columns that contain missing data. However, this approach can lead to a loss of information and bias in the dataset\n",
    "2) Impute missing data : This involves filling in missing data with an estimated value. There are several methods for imputing missing data, including mean imputation, median imputation, mode imputation, and hot-deck imputation\n",
    "3) Use predictive models : This involves using predictive models, such as regression or decision trees, to estimate missing values based on the values of other variables\n",
    "4) Multiple imputation : This involves creating multiple imputed datasets by imputing missing data several times and then analyzing each imputed dataset separately. The results are then combined to generate a final estimate.\n",
    "\n",
    "5) Use specialized techniques: There are several specialized techniques for handling missing data, including k-nearest neighbors (KNN) imputation, expectation-maximization (EM) algorithm, and Bayesian imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb7e15-f4cc-4da9-b0e2-db179a7abad3",
   "metadata": {},
   "source": [
    "8) You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa74dbb-7afe-42f5-9d08-e49b55425f25",
   "metadata": {},
   "source": [
    "When working with a large dataset with missing data, it is important to determine whether the missing data is missing completely at random (MCAR), missing at random (MAR), or missing not at random (MNAR). Here are some strategies to help determine the pattern of missing data:\n",
    "\n",
    "1) Visualize missingness: Plot the missing data pattern to visualize any missingness. This can be done using heatmaps, dendrograms, or bar plots.\n",
    "\n",
    "2) Compare missing data patterns across variables: Compare the missing data pattern across different variables to see if there are any patterns or associations.\n",
    "\n",
    "3) Conduct statistical tests: Conduct statistical tests to determine if there is a relationship between the missing data and other variables in the dataset. This can be done using chi-square tests or logistic regression.\n",
    "\n",
    "4) Use imputation methods: Use imputation methods to impute missing data and then compare the results of the analysis with and without imputation. If the results are similar, the missing data may be MCAR or MAR.\n",
    "\n",
    "5) Consult domain experts: Consult domain experts to gain insight into the nature of the data and to identify any potential sources of bias or missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d381a68-7a2d-4a89-a8b1-08571be8cefe",
   "metadata": {},
   "source": [
    "9) Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6d3972-c496-4bf0-8e02-d3837c2dd4f7",
   "metadata": {},
   "source": [
    "When working with an imbalanced dataset, such as in a medical diagnosis project where the majority of patients do not have the condition of interest, there are several strategies to evaluate the performance of a machine learning model:\n",
    "\n",
    "1) Confusion Matrix: Use a confusion matrix to calculate metrics such as accuracy, precision, recall, and F1-score. These metrics provide an overall evaluation of the performance of the model.\n",
    "\n",
    "2) ROC Curve: Plot an ROC curve and calculate the area under the curve (AUC) to evaluate the performance of the model.\n",
    "\n",
    "3) Cost-Sensitive Learning: Assign different weights to the positive and negative classes to reflect the cost of misclassification. This can be done using techniques such as undersampling, oversampling, or using synthetic data generation techniques like SMOTE.\n",
    "\n",
    "4) Ensemble Techniques: Use ensemble techniques such as bagging, boosting, or stacking to combine multiple models and improve performance.\n",
    "\n",
    "5) Threshold Adjustment: Adjust the decision threshold of the model to balance the trade-off between precision and recall.\n",
    "\n",
    "6) Stratified Sampling: Use stratified sampling during cross-validation to ensure that the distribution of the target variable is preserved in the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8867a56d-bf9c-4b7c-a81c-053aeba8dbcd",
   "metadata": {},
   "source": [
    "10) When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88edcba1-9b5a-46b6-92ab-d6b07669ee77",
   "metadata": {},
   "source": [
    "1) Random Under-Sampling: This method involves randomly removing samples from the majority class to balance the dataset. However, this method may result in the loss of valuable information.\n",
    "\n",
    "2) Tomek Links: This method involves identifying samples from the majority class that are close to samples from the minority class and removing them. This can help to remove noise from the majority class and improve the separation between the classes.\n",
    "\n",
    "3) Cluster-Based Under-Sampling: This method involves clustering samples from the majority class and removing clusters that are far from the clusters of the minority class. This method can be effective in removing redundant samples and retaining informative ones.\n",
    "\n",
    "4) Synthetic Minority Over-Sampling Technique (SMOTE): This method involves generating synthetic samples for the minority class by interpolating between the existing samples. This method can increase the size of the minority class and improve the separation between the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71644f10-8a60-4d34-9abd-501da6e306ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "df_majority = df[df.satisfaction=='satisfied']\n",
    "df_minority = df[df.satisfaction=='not satisfied']\n",
    "df_majority_downsampled = resample(df_majority,\n",
    "                                   replace=False,\n",
    "                                   n_samples=len(df_minority),\n",
    "                                   random_state=42)\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "print(df_downsampled.satisfaction.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873aeb6-c5b0-4b8d-b8e8-dd7b97893809",
   "metadata": {},
   "source": [
    "11) You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a2e20-5124-49d8-8f64-a4c385c5fced",
   "metadata": {},
   "source": [
    "1) Random Over-Sampling: This method involves randomly replicating samples from the minority class to increase its size. However, this method may result in overfitting and the loss of valuable information.\n",
    "\n",
    "2) Synthetic Minority Over-Sampling Technique (SMOTE): This method involves generating synthetic samples for the minority class by interpolating between the existing samples. This method can increase the size of the minority class and improve the separation between the classes.\n",
    "\n",
    "3) Adaptive Synthetic Sampling (ADASYN): This method is an extension of SMOTE that generates more synthetic samples for samples that are harder to learn by the classifier. This can improve the robustness of the classifier and reduce overfitting.\n",
    "\n",
    "4) Cluster-Based Over-Sampling: This method involves clustering samples from the minority class and generating new samples in the clusters. This can help to generate more diverse synthetic samples and avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd23e20-7b7d-4ea2-b8d4-28cf44cdfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "df_majority = df[df.target==0]\n",
    "df_minority = df[df.target==1]\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_resample(X, y)\n",
    "print(y_res.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
